# Práctica 6: Localización Visual Absoluta mediante AprilTags y Estimación de Pose 3D

## Desarrollo del código y explicación en profundidad

El objetivo principal de esta práctica es implementar un sistema de localización visual y navegación autónoma para un robot móvil dentro de un entorno, combinando percepción visual (AprilTags), odometría y navegación reactiva.

El sistema permite al robot estimar su pose absoluta en el mundo utilizando balizas conocidas, corrigiendo así el error acumulado de la odometría.

Y cuando no hay referencias visuales nuestro sistema principal de apoyo es medir los incrementos de posición respecto de la odometría. Anotando la última posición en que vimos una baliza. 
Nuestra nueva estimación va a ser nuestra estimación anterior, más la suma respecto de la odometría. De esta manera evitamos "teletransportes", es decir, saltos brucos en la posición debido a no saber realmente donde nos encontramos. 


## Enfoque general de la solución

La solución implementada sigue un enfoque híbrido:

  - Odometría global como base continua de movimiento.

  - Correcciones visuales absolutas mediante AprilTags cuando están disponibles.

  - Navegación reactiva basada en láser para avanzar o girar.

Elegí este enfoque tras comprobar que:

  - La odometría por sí sola deriva con el tiempo.

  - La visión por sí sola es discontinua y ruidosa.

  - Integrar incrementalmente la odometría encima de una estimación visual producía errores graves de referencia.


## Configuración, parámetros y decisiones físicas

**Posiciones conocidas de los tags**

Cada tag tiene una posición absoluta conocida en el mundo. Incluye (x, y, z, yaw).
Esto permite tratar cada detección como una baliza absoluta, no relativa.

La orientación (yaw) del tag es especialmente importante, ya que los tags están colocados en paredes y definen su propio sistema de referencia.


**Offset de la cámara**

La cámara no está situada en el centro geométrico del robot, sino adelantada.

Si este offset no se aplica:

  - Hallaremos errores sistemáticos de localización y para determinar a que distancia nos encontramos de las balizas.

Teniendo en cuenta que dicho offset se aplica después de obtener la pose de la cámara, nunca antes.


**Estado interno del robot (RobotState)**

Decisión clave: odometría global directa

La odometría que nos proporciona HAL ya está en el sistema de referencia global. Por tanto, no se debe volver a integrar incrementalmente con cosenos y senos.


En versiones anteriores, calculaban deltas de odometría y los rotaba con cos(yaw) y sin(yaw).

Esto provocaba que inicialmente el avance pareciera correcto pero a partir de cierto punto, la estimación empezaba a retroceder ficticiamente.

La posición estimada oscilaba entre dos valores (ej. 2.09 ↔ 1.07).


**Calibración de la cámara**

Durante las pruebas observé que si calculaba la focal dinámicamente se producían colapsos de escala.

Con una focal fija:

  - La profundidad es estable.

  - El sistema PnP converge correctamente.


**Transformación de coordenadas**

solvePnP devuelve:

  - rvec: orientación del tag respecto a la cámara.

  - tvec: posición del tag en el frame de la cámara.

Esto NO es todavía la posición del robot. En mi caso fue crítico depurar la conversión de la cámara al robot. 


En OpenCV: Z apunta hacia delante de la cámara.

En el robot: El eje X apunta hacia delante.


Por tanto: z pasa a ser x del robot, x pasa a ser la y e y pasa a ser la z.

De esta manera la conversión del tag al mundo equivale a una matriz de rotación, evitando así construir matrices homogéneas completas innecesarias.

**Promedio de múltiples balizas**

Si se detectan varios tags simultáneamente:

Se calcula un promedio de posiciones, y se hace el promedio circular del yaw.

De manera que reducimos el ruido, aumentando robustez, y evitando que una sola detección errónea domine.

## Vídeo de la ejecución

https://github.com/user-attachments/assets/97a57e87-a4c4-4341-bfcf-3e9bfa37d00e


## Licencia
This work is submitted under the CC-BY-SA 4.0 license, meaning it can be shared and adapted as long as it is properly attributed and shared under the same terms.

## Autor
Marina Antolínez Cabrero
